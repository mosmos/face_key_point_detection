{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"key_point_2.ipynb","provenance":[{"file_id":"1APrpLKjygPrlGXjymCVBX6HdBQlpOkse","timestamp":1568748795809}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ctIM74BY9Ano","colab_type":"code","colab":{}},"source":["'''\n","my version of the paper https://arxiv.org/pdf/1710.00977.pdf\n","\n","\n","\n","IN :1x224x224\n","68 face keypoint\n","\n","'''\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pdcDK09LxlPl","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6LuMdmyTDkxr","colab_type":"code","colab":{}},"source":["import sys\n","sys.path.append(\"/content/drive/My Drive/key_point\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"68de9535psI9","colab_type":"code","colab":{}},"source":["!mkdir /data\n","!wget -P /data/ https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip\n","!unzip -n /data/train-test-data.zip -d /data"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UxdS7KiUrIpH","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.nn.init as I\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","#from model_NaimishNet import Net\n","from model_NaimishNet_3 import Net\n","\n","\n","net = Net()\n","print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpYVQBhdrwBN","colab_type":"code","colab":{}},"source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","# the dataset we created in Notebook 1 is copied in the helper file `data_load.py`\n","from data_load import FacialKeypointsDataset\n","# the transforms we defined in Notebook 1 are in the helper file `data_load.py`\n","from data_load import Rescale, RandomCrop, Normalize, ToTensor\n","\n","\n","## TODO: define the data_transform using transforms.Compose([all tx's, . , .])\n","# order matters! i.e. rescaling should come before a smaller crop\n","data_transform = transforms.Compose([ Rescale(250), RandomCrop(224),  Normalize(), ToTensor()]) # no cropping in the paper\n","\n","# testing that you've defined a transform\n","assert(data_transform is not None), 'Define a data_transform'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rKylsoprwHM","colab_type":"code","colab":{}},"source":["# create the transformed dataset\n","transformed_dataset = FacialKeypointsDataset(csv_file='/data/training_frames_keypoints.csv',\n","                                             root_dir='/data/training/',\n","                                             transform=data_transform)\n","\n","\n","print('Number of images: ', len(transformed_dataset))\n","\n","# iterate through the transformed dataset and print some stats about the first few samples\n","for i in range(4):\n","    sample = transformed_dataset[i]\n","    print(i, sample['image'].size(), sample['keypoints'].size())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FHtkv7dXBF2-","colab_type":"code","colab":{}},"source":["transformed_dataset.key_pts_frame.columns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3DBObt3zCN-","colab_type":"code","colab":{}},"source":["batch_size = 30\n","# load training data in batches\n","train_loader = DataLoader(transformed_dataset, \n","                          batch_size=batch_size,\n","                          shuffle=True, \n","                          num_workers=4)\n","\n","\n","# create the test dataset\n","test_dataset = FacialKeypointsDataset(csv_file='/data/test_frames_keypoints.csv',\n","                                             root_dir='/data/test/',\n","                                             transform=data_transform)\n","\n","# load test data in batches\n","test_loader = DataLoader(test_dataset, \n","                          batch_size=batch_size,\n","                          shuffle=True, \n","                          num_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8ZEszBfzCRM","colab_type":"code","colab":{}},"source":["# test the model on a batch of test images\n","\n","def net_sample_output():\n","    \n","    # iterate through the test dataset\n","    for i, sample in enumerate(test_loader):\n","        \n","        # get sample data: images and ground truth keypoints\n","        images = sample['image']\n","        key_pts = sample['keypoints']\n","\n","        # convert images to FloatTensors\n","        images = images.type(torch.FloatTensor)\n","        print (\"images.shape\", images.shape)\n","        # forward pass to get net output\n","        output_pts = net(images)\n","        \n","        print (\"output_pts.shape\", output_pts.shape)\n","        # reshape to batch_size x 68 x 2 pts\n","        output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n","        \n","        # break after first image is tested\n","        if i == 0:\n","            return images, output_pts, key_pts\n","            "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2FVps0MZzCUe","colab_type":"code","colab":{}},"source":["#net_sample_output()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMG0JJeAzCXO","colab_type":"code","colab":{}},"source":["test_images, test_outputs, gt_pts = net_sample_output()\n","\n","# print out the dimensions of the data to see if they make sense\n","print(test_images.data.size())\n","print(test_outputs.data.size())\n","print(gt_pts.size())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v-wl6JUJzCaB","colab_type":"code","colab":{}},"source":["def show_all_keypoints(image, predicted_key_pts, gt_pts=None):\n","    \"\"\"Show image with predicted keypoints\"\"\"\n","    # image is grayscale\n","    plt.imshow(image, cmap='gray')\n","    plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='m')\n","    # plot ground truth points as green pts\n","    if gt_pts is not None:\n","        plt.scatter(gt_pts[:, 0], gt_pts[:, 1], s=20, marker='x', c='g')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_WrGZUQrzCcf","colab_type":"code","colab":{}},"source":["# visualize the output\n","# by default this shows a batch of 10 images\n","def visualize_output(test_images, test_outputs, gt_pts=None, batch_size=10):\n","\n","    for i in range(batch_size):\n","        plt.figure(figsize=(20,10))\n","        ax = plt.subplot(1, batch_size, i+1)\n","\n","        # un-transform the image data\n","        image = test_images[i].data   # get the image from it's Variable wrapper\n","        image = image.numpy()   # convert to numpy array from a Tensor\n","        image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n","\n","        # un-transform the predicted key_pts data\n","        predicted_key_pts = test_outputs[i].data\n","        predicted_key_pts = predicted_key_pts.numpy()\n","        # undo normalization of keypoints  \n","        predicted_key_pts = predicted_key_pts*50.0+100\n","        \n","        # plot ground truth points for comparison, if they exist\n","        ground_truth_pts = None\n","        if gt_pts is not None:\n","            ground_truth_pts = gt_pts[i]         \n","            ground_truth_pts = ground_truth_pts*50.0+100\n","        \n","        # call show_all_keypoints\n","        show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n","            \n","        plt.axis('off')\n","\n","    plt.show()\n","    \n","    \n","# call it\n","visualize_output(test_images, test_outputs, gt_pts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_20qacvezCe3","colab_type":"code","colab":{}},"source":["import torch.optim as optim\n","\n","criterion = nn.MSELoss()\n","\n","optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-06)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dHl-7_33nEP","colab_type":"code","colab":{}},"source":["def train_net(n_epochs):\n","\n","    # prepare the net for training\n","    net.train()\n","\n","    for epoch in range(n_epochs):  # loop over the dataset multiple times\n","        \n","        running_loss = 0.0\n","\n","        # train on batches of data, assumes you already have train_loader\n","        for batch_i, data in enumerate(train_loader):\n","            # get the input images and their corresponding labels\n","            images  = data['image']\n","            key_pts = data['keypoints']\n","\n","            # flatten pts\n","            key_pts = key_pts.view(key_pts.size(0), -1)\n","\n","            # convert variables to floats for regression loss\n","            key_pts = key_pts.type(torch.FloatTensor)\n","            images  = images.type(torch.FloatTensor)\n","\n","            # forward pass to get outputs\n","            output_pts = net(images)\n","\n","            # calculate the loss between predicted and target keypoints\n","            loss = criterion(output_pts, key_pts)\n","\n","            # zero the parameter (weight) gradients\n","            optimizer.zero_grad()\n","            \n","            # backward pass to calculate the weight gradients\n","            loss.backward()\n","\n","            # update the weights\n","            optimizer.step()\n","\n","            # print loss statistics\n","            running_loss += loss.item()\n","            print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n","            if batch_i % 10 == 9:    # print every 10 batches\n","                print('Epoch: {}, Batch: {}, Avg. Loss: {}'.format(epoch + 1, batch_i+1, running_loss/10))\n","                running_loss = 0.0\n","\n","    print('Finished Training')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWhhMSqf3nHA","colab_type":"code","colab":{}},"source":["# train your network\n","n_epochs =20 # start small, and increase when you've decided on your model structure and hyperparams shuld be 300\n","\n","# this is a Workspaces-specific context manager to keep the connection\n","# alive while training your model, not part of pytorch\n","#with active_session():\n","train_net(n_epochs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sj9FFdCg3nJx","colab_type":"code","colab":{}},"source":["# get a sample of test data again\n","test_images, test_outputs, gt_pts = net_sample_output()\n","\n","print(test_images.data.size())\n","print(test_outputs.data.size())\n","print(gt_pts.size())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QKEp-hw43nMc","colab_type":"code","colab":{}},"source":["## TODO: visualize your test output\n","# you can use the same function as before, by un-commenting the line below:\n","\n","visualize_output(test_images, test_outputs, gt_pts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fLMUp6ZM3nPR","colab_type":"code","colab":{}},"source":["## TODO: change the name to something uniqe for each new model\n","model_dir = \"/content/drive/My Drive/key_point/\"\n","model_name = 'keypoints_model_20190927_7x7_epocs10_btc30_lr0001.pt'\n","\n","# after training, save your model parameters in the dir 'saved_models'\n","torch.save(net.state_dict(), model_dir+model_name)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-62_kkMB3nRf","colab_type":"code","colab":{}},"source":["# Get the weights in the first conv layer, \"conv1\"\n","# if necessary, change this to reflect the name of your first conv layer\n","weights1 = net.conv1.weight.data\n","\n","w = weights1.numpy()\n","\n","filter_index = 0\n","\n","print(w[filter_index][0])\n","print(w[filter_index][0].shape)\n","\n","# display the filter weights\n","plt.imshow(w[filter_index][0], cmap='gray')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bqy10p3tN5Ne","colab_type":"code","colab":{}},"source":["import cv2 as cv\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KmkZZ1O_3nUf","colab_type":"code","colab":{}},"source":["##TODO: load in and display any image from the transformed test dataset\n","\n","## TODO: Using cv's filter2D function,\n","## apply a specific set of filter weights (like the one displayed above) to the test image\n","\n","\n","def w_filter(img,kernel):\n","    \n","  #img = cv.imread(cv.samples.findFile(imageName), cv.IMREAD_COLOR)\n","\n","    \n","    \n","  ind = 0\n","\n","  #kernel_size = 3 + 2 * (ind % 5)\n","  #kernel = np.ones((kernel_size, kernel_size), dtype=np.float32)\n","  #kernel /= (kernel_size * kernel_size)\n","\n","  dst = cv.filter2D(img, -1, kernel)\n","\n","  return dst\n","  \n","\n","  \n","img_num = 9  \n","img    = test_images[img_num].data.numpy()\n","\n","kernel = net.conv1.weight.data.numpy()[0][0] *0.1\n","\n","#kernel = np.ones((3,3),np.float32)  #testing on simple kerenl\n","#kernel[1,1] = 8\n","\n","filtered_img = w_filter(img,kernel)\n","\n","plt.figure(figsize=(20,10))\n","imgs = [kernel, img[0,:,:], filtered_img[0,:,:] ]\n","for i in range(len(imgs)):\n","  ax = plt.subplot(1, 3, i+1)\n","  plt.imshow(imgs[i], cmap='gray')\n","  plt.axis('off')\n","plt.show\n","\n","\n","for i in range(len(imgs)):\n","  print (imgs[i].shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0f1qooJ-Nf4l","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSELx55hIykZ","colab_type":"code","colab":{}},"source":["i=5\n","batch_size = 7\n","plt.figure(figsize=(75,25))\n","ax = plt.subplot(1, batch_size, i+1)\n","\n","# un-transform the image data\n","image = test_images[i].data   # get the image from it's Variable wrapper\n","image = image.numpy()   # convert to numpy array from a Tensor\n","image = np.transpose(image, (1, 2, 0))   # transpose to go from torch to numpy image\n","\n","# un-transform the predicted key_pts data\n","predicted_key_pts = test_outputs[i].data\n","predicted_key_pts = predicted_key_pts.numpy()\n","# undo normalization of keypoints  \n","predicted_key_pts = predicted_key_pts*50.0 + 100\n","        \n","# plot ground truth points for comparison, if they exist\n","ground_truth_pts = None\n","if gt_pts is not None:\n","  ground_truth_pts = gt_pts[i]         \n","  ground_truth_pts = ground_truth_pts*50.0+100\n","        \n","# call show_all_keypoints\n","show_all_keypoints(np.squeeze(image), predicted_key_pts, ground_truth_pts)\n","            \n","#plt.axis('off')\n","\n","plt.show()"],"execution_count":0,"outputs":[]}]}
